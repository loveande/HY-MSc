{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# # 解析XML文件\n",
    "# tree = ET.parse('NLPCC2014微博情绪分析样例数据.xml')\n",
    "# root = tree.getroot()\n",
    "\n",
    "# data = []\n",
    "\n",
    "# # 遍历weibo标签\n",
    "# for weibo in root.iter('weibo'):\n",
    "#     label = weibo.attrib['emotion-type1']\n",
    "#     # 遍历sentence标签\n",
    "#     for sentence in weibo.iter('sentence'):\n",
    "#         content = sentence.text\n",
    "#         # 将数据添加到列表中\n",
    "#         data.append({'Sentence': content, 'Emotion': label})\n",
    "\n",
    "# # 创建DataFrame对象\n",
    "# df = pd.DataFrame(data)\n",
    "# df.Emotion.value_counts()\n",
    "# use_lab = ['like','sadness','anger']\n",
    "# df = df[df['Emotion'].isin(use_lab)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新的数据集\n",
    "# https://www.kaggle.com/datasets/honyuu/chnsenticorp-htl-all?resource=download\n",
    "df = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['review','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review    object\n",
       "label      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zhangyu\\.conda\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 数据预处理 and 获取句子语义特征\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')\n",
    "model = AutoModel.from_pretrained('bert-base-chinese')\n",
    "device = 'cuda'\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 7766/7766 [03:19<00:00, 38.97it/s]\n"
     ]
    }
   ],
   "source": [
    "sen_embs = []\n",
    "labels = []\n",
    "\n",
    "for sen, lab in tqdm(df.to_numpy(),ncols=80):\n",
    "    # 文本预处理\n",
    "    if isinstance(lab,int) and isinstance(sen, str):\n",
    "        tokens, mask, t_type = tokenizer(sen, \n",
    "                padding=\"max_length\", \n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "                ).values()\n",
    "        # 抽取文本语义特征\n",
    "        sen_emb = model(\n",
    "                        input_ids=tokens.to(device),\n",
    "                        token_type_ids=mask.to(device),\n",
    "                        attention_mask=t_type.to(device)\n",
    "                        )['pooler_output']\n",
    "        sen_embs.append(sen_emb.detach().cpu().numpy())\n",
    "        labels.append(lab)\n",
    "\n",
    "sen_embs = np.concatenate(sen_embs, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 7765/7765 [02:24<00:00, 53.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# 根据语义相似度构建图\n",
    "k = 3 # 每个节点的邻居数量\n",
    "# 为每个节点找到最相似的k个节点作为图中的邻居节点\n",
    "neighbors = []\n",
    "\n",
    "def cosine_similarity(vector, matrix):\n",
    "    # 计算向量的范数\n",
    "    vector_norm = np.linalg.norm(vector)\n",
    "    # 计算矩阵每个样本的范数\n",
    "    matrix_norm = np.linalg.norm(matrix, axis=1)\n",
    "    # 计算向量和矩阵每个样本的内积\n",
    "    dot_product = np.dot(matrix, vector)\n",
    "    # 计算余弦相似度\n",
    "    similarity = dot_product / (matrix_norm * vector_norm)\n",
    "    return similarity\n",
    "\n",
    "\n",
    "for sen in tqdm(sen_embs, ncols=80):\n",
    "    # 计算语义相似度\n",
    "    similarity = cosine_similarity(sen, sen_embs) \n",
    "    # 找到节点的邻居\n",
    "    neis = similarity.argsort()[-k-1:]\n",
    "    # 保存邻居信息\n",
    "    neighbors.append(neis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 7765/7765 [00:01<00:00, 3891.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# 图特征提取\n",
    "import networkx as nx\n",
    "from node2vec import Node2Vec\n",
    "\n",
    "# 创建图结构\n",
    "G = nx.Graph()\n",
    "for tail, neis in enumerate(neighbors):\n",
    "    for head in neis:\n",
    "        G.add_edge(head, tail)\n",
    "# 使用 Node2Vec 训练节点嵌入向量\n",
    "node2vec = Node2Vec(G, dimensions=32, walk_length=30, num_walks=200, workers=4)\n",
    "# 拟合模型（生成随机游走序列）\n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "# 获取节点的嵌入向量\n",
    "node_embeddings = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将图特征和图结构保存\n",
    "node_graph_feat = []\n",
    "for i in range(len(sen_embs)):\n",
    "    node_graph_feat.append(node_embeddings[i])\n",
    "node_graph_feat = np.stack(node_graph_feat, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = np.arange(len(node_graph_feat)).reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将label数字化\n",
    "label2int = dict((l,idx) for idx, l in enumerate(np.unique((labels))))\n",
    "label_int = [label2int[l] for l in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集 测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 使用train_test_split函数划分数据集\n",
    "X_train, X_test, y_train, y_test = train_test_split(nodes, label_int, test_size=0.2, stratify=label_int, random_state=42)\n",
    "X_train = X_train.reshape(-1)\n",
    "X_test = X_test.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump((neighbors,node_graph_feat), open('graph_data.pkl','wb'))\n",
    "pickle.dump((X_train, X_test, y_train, y_test), open('dataset.pkl','wb'))\n",
    "pickle.dump(sen_embs, open('sen_embs.pkl','wb'))\n",
    "pickle.dump((label2int,labels), open('label2int.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
